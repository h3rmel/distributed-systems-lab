---
description: Engineering patterns, SOLID principles, and code quality standards for all projects in the monorepo
globs:
  - "**/*.ts"
  - "**/*.tsx"
  - "**/*.js"
  - "**/*.jsx"
---

# System Patterns & Engineering Standards

## Universal Principles (All Projects)

### 1. SOLID Principles (Mandatory)

#### Single Responsibility Principle (SRP)
- **Files:** Each file should contain one class/interface with a single purpose
- **Functions:** Each function should perform one well-defined operation
- **Modules:** Each NestJS module handles one bounded context
- **Splitting Rule:** If a file exceeds 300 lines or a function exceeds 50 lines, refactor into smaller units

#### Open/Closed Principle
- Use interfaces and dependency injection for extensibility
- Add new features by extending, not modifying existing code
- Example: New webhook providers should extend `BaseWebhookProvider`, not modify the controller

#### Liskov Substitution Principle
- Subtypes must be substitutable for their base types
- Implementations must honor interface contracts completely

#### Interface Segregation Principle
- Define small, focused interfaces rather than large, monolithic ones
- Clients should not depend on interfaces they don't use

#### Dependency Inversion Principle
- Depend on abstractions (interfaces), not concrete implementations
- Use NestJS dependency injection for all service dependencies

### 2. TypeScript Strict Mode (Enforced)

#### Type Safety Rules
```typescript
// ✅ GOOD: Explicit types everywhere
interface WebhookJobData {
  provider: string;
  eventId: string;
  timestamp: string;
  data: Record<string, unknown>;
}

async function processWebhook(job: Job<WebhookJobData>): Promise<Result<void, ProcessingError>> {
  // Implementation
}

// ❌ BAD: Using 'any'
async function processWebhook(job: any): Promise<any> {
  // This is not acceptable
}
```

#### No Implicit Any
- **tsconfig.json:** `"noImplicitAny": true` must be enabled
- **Rule:** Never use `any` unless absolutely unavoidable and explicitly justified with a comment
- **Alternative:** Use `unknown` and perform type guards when dynamic types are required

#### Strict Null Checks
- **tsconfig.json:** `"strictNullChecks": true` must be enabled
- Always handle `null` and `undefined` explicitly
- Use optional chaining (`?.`) and nullish coalescing (`??`)

### 3. Error Handling Strategy (Dual Pattern)

#### Result Pattern (Business Logic)

Use the Result Pattern for expected failures in business logic:

```typescript
// Define the Result type
type Result<T, E = Error> = 
  | { success: true; value: T }
  | { success: false; error: E };

// Usage example
async function validateWebhook(data: unknown): Promise<Result<WebhookJobData, ValidationError>> {
  if (!isValidWebhookData(data)) {
    return {
      success: false,
      error: new ValidationError('Invalid webhook structure')
    };
  }
  
  return {
    success: true,
    value: data as WebhookJobData
  };
}

// Consumer must handle both cases
const result = await validateWebhook(payload);
if (!result.success) {
  // Handle validation failure
  return { accepted: false, reason: result.error.message };
}
// Continue with result.value
```

**When to use Result Pattern:**
- Validation failures
- Resource not found scenarios
- Business rule violations
- Idempotency check failures
- Rate limit exceeded

#### Try/Catch Pattern (Execution Boundaries)

Use try/catch at execution boundaries for unexpected errors:

```typescript
// Controller (execution boundary)
@Post(':provider')
async ingest(
  @Param('provider') provider: string,
  @Body() dto: CreateWebhookDto,
): Promise<{ accepted: boolean; jobId?: string }> {
  try {
    const jobId = await this.webhookService.enqueue(provider, dto);
    return { accepted: true, jobId };
  } catch (error) {
    this.logger.error('Unexpected error during webhook ingestion', error);
    throw new InternalServerErrorException('Failed to process webhook');
  }
}

// Worker (execution boundary)
@Process('ingest')
async handleJob(job: Job<WebhookJobData>): Promise<void> {
  try {
    await this.processWebhookJob(job.data);
  } catch (error) {
    this.logger.error('Worker processing failed', { jobId: job.id, error });
    throw error; // BullMQ will handle retry logic
  }
}
```

**When to use Try/Catch:**
- API route handlers
- Event listeners
- Worker processors
- Top-level application initialization
- Database connection establishment

### 4. Asynchronous I/O (Non-Blocking)

#### Mandatory Patterns
```typescript
// ✅ GOOD: All I/O is async/await
async function saveWebhook(data: WebhookJobData): Promise<void> {
  await db.insert(webhooksTable).values(data);
}

async function checkIdempotency(eventId: string): Promise<boolean> {
  const exists = await redis.get(`webhook:${eventId}`);
  return exists !== null;
}

// ❌ BAD: Synchronous I/O
function saveWebhookSync(data: WebhookJobData): void {
  // Never use synchronous database operations
  db.insertSync(webhooksTable).values(data);
}
```

#### Rules
- All database operations must be async
- All Redis operations must be async
- All external API calls must be async
- All file I/O operations must be async
- Use `Promise.all()` for parallel independent operations

#### Performance Optimization
```typescript
// ✅ GOOD: Parallel execution
async function enrichWebhookData(eventId: string): Promise<EnrichedData> {
  const [user, metadata, config] = await Promise.all([
    fetchUser(eventId),
    fetchMetadata(eventId),
    fetchConfig(eventId),
  ]);
  
  return { user, metadata, config };
}

// ❌ BAD: Sequential execution when not needed
async function enrichWebhookDataSlow(eventId: string): Promise<EnrichedData> {
  const user = await fetchUser(eventId);
  const metadata = await fetchMetadata(eventId);
  const config = await fetchConfig(eventId);
  
  return { user, metadata, config };
}
```

### 5. NestJS Architectural Patterns

#### Controller Responsibility
**Rule:** Controllers must ONLY:
- Validate DTOs using `class-validator`
- Call service methods
- Return HTTP responses

**Controllers must NEVER:**
- Contain business logic
- Perform database operations directly
- Make external API calls
- Process data transformations

```typescript
// ✅ GOOD: Thin controller
@Controller('webhooks')
export class WebhookController {
  constructor(private readonly webhookService: WebhookService) {}
  
  @Post(':provider')
  async ingest(
    @Param('provider') provider: string,
    @Body() dto: CreateWebhookDto,
  ): Promise<IngestResponseDto> {
    return this.webhookService.enqueue(provider, dto);
  }
}

// ❌ BAD: Fat controller with business logic
@Controller('webhooks')
export class WebhookController {
  @Post(':provider')
  async ingest(@Body() dto: CreateWebhookDto): Promise<any> {
    // Never do this in controllers!
    const isDuplicate = await redis.get(`webhook:${dto.eventId}`);
    if (isDuplicate) return { accepted: false };
    
    await db.insert(webhooksTable).values(dto);
    return { accepted: true };
  }
}
```

#### Service Responsibility
- Contain business logic
- Orchestrate between multiple providers
- Handle Result Pattern returns
- Perform data transformations

#### Module Separation
- **WebhookModule:** API ingress (Controllers + Queue injection)
- **WorkerModule:** Background processing (Processors + Database logic)
- **SharedModule:** Type definitions, interfaces, constants
- **HealthModule:** Health checks and monitoring
- **DatabaseModule:** ORM configuration and connection management

### 6. Code Documentation Standards

#### JSDoc/TSDoc Requirements
```typescript
/**
 * Enqueues a webhook event for asynchronous processing.
 * 
 * @param provider - The webhook provider identifier (e.g., 'stripe', 'paypal')
 * @param dto - Validated webhook payload conforming to CreateWebhookDto
 * @returns Promise resolving to job metadata with unique job ID
 * @throws {QueueConnectionError} When Redis connection is unavailable
 * 
 * @example
 * ```typescript
 * const result = await webhookService.enqueue('stripe', webhookDto);
 * console.log(`Job enqueued with ID: ${result.jobId}`);
 * ```
 */
async enqueue(
  provider: string,
  dto: CreateWebhookDto,
): Promise<EnqueueResult> {
  // Implementation
}
```

**Documentation Required For:**
- All exported functions
- All public class methods
- All interfaces and types (especially shared ones)
- Complex internal logic (algorithms, business rules)

### 7. Data Structure Efficiency

#### Use Appropriate Collections
```typescript
// ✅ GOOD: O(1) lookup with Map
const providerHandlers = new Map<string, WebhookHandler>([
  ['stripe', stripeHandler],
  ['paypal', paypalHandler],
]);

const handler = providerHandlers.get(provider);

// ❌ BAD: O(n) lookup with Array
const providerHandlers = [
  { name: 'stripe', handler: stripeHandler },
  { name: 'paypal', handler: paypalHandler },
];

const handler = providerHandlers.find(p => p.name === provider)?.handler;
```

#### Rules
- Use `Map` for key-value lookups (not plain objects for dynamic keys)
- Use `Set` for uniqueness checks (not Array.includes())
- Use `Array.filter()` sparingly on large datasets
- Consider caching results of expensive computations

### 8. Logging Standards

#### Structured Logging (nestjs-pino)
```typescript
// ✅ GOOD: Structured logs with context
this.logger.info('Webhook enqueued', {
  provider,
  eventId: dto.eventId,
  jobId,
  timestamp: new Date().toISOString(),
});

// ❌ BAD: Unstructured string logs
this.logger.info(`Webhook from ${provider} enqueued with job ${jobId}`);
```

#### Log Levels
- **error:** Unexpected failures, system errors
- **warn:** Expected failures, validation errors, rate limits
- **info:** Business events (job enqueued, job completed)
- **debug:** Development debugging (disabled in production)

---

## Project-Specific Patterns

### 9. Next.js Dashboard Patterns (`live-dashboard`)

#### React Performance Optimization

**CRITICAL:** Do not use `useEffect` for high-frequency socket listeners.

```typescript
// ✅ GOOD: Zustand store for high-frequency updates
import { create } from 'zustand';

interface MetricsStore {
  events: WebhookEvent[];
  rps: number;
  addEvent: (event: WebhookEvent) => void;
}

export const useMetricsStore = create<MetricsStore>((set) => ({
  events: [],
  rps: 0,
  addEvent: (event) => set((state) => ({
    events: [event, ...state.events].slice(0, 100), // Keep last 100
  })),
}));

// Socket listener OUTSIDE React component
socket.on('job-completed', (event) => {
  useMetricsStore.getState().addEvent(event);
});

// ❌ BAD: useEffect creates re-render storms
function Dashboard() {
  const [events, setEvents] = useState([]);
  
  useEffect(() => {
    socket.on('job-completed', (event) => {
      setEvents(prev => [...prev, event]); // Re-renders on EVERY event
    });
  }, []);
}
```

#### Throttled UI Updates

```typescript
// ✅ GOOD: Throttle chart updates to 30-60 FPS
import { useEffect, useState } from 'react';
import { useMetricsStore } from './store';

export function ThroughputChart() {
  const [chartData, setChartData] = useState([]);
  
  useEffect(() => {
    // Update UI at most 30 times per second
    const interval = setInterval(() => {
      const latestMetrics = useMetricsStore.getState().calculateRPS();
      setChartData(latestMetrics);
    }, 1000 / 30); // 30 FPS
    
    return () => clearInterval(interval);
  }, []);
  
  return <LineChart data={chartData} />;
}

// ❌ BAD: Direct subscription causes re-render on every event
export function ThroughputChart() {
  const events = useMetricsStore(state => state.events); // Re-renders 100+ times/sec
  return <LineChart data={events} />;
}
```

#### Virtualized Lists (Mandatory for Live Logs)

```typescript
// ✅ GOOD: Use TanStack Virtual for large lists
import { useVirtualizer } from '@tanstack/react-virtual';

export function LiveLogStream() {
  const events = useMetricsStore(state => state.events);
  const parentRef = useRef<HTMLDivElement>(null);
  
  const virtualizer = useVirtualizer({
    count: events.length,
    getScrollElement: () => parentRef.current,
    estimateSize: () => 50, // Row height
  });
  
  return (
    <div ref={parentRef} style={{ height: '600px', overflow: 'auto' }}>
      <div style={{ height: `${virtualizer.getTotalSize()}px` }}>
        {virtualizer.getVirtualItems().map(virtualRow => (
          <div key={virtualRow.key} style={{ height: `${virtualRow.size}px` }}>
            {events[virtualRow.index].eventId}
          </div>
        ))}
      </div>
    </div>
  );
}

// ❌ BAD: Standard <ul> will freeze browser with 100+ rapid updates
export function LiveLogStream() {
  const events = useMetricsStore(state => state.events);
  return (
    <ul>
      {events.map(event => <li key={event.id}>{event.eventId}</li>)}
    </ul>
  );
}
```

#### Socket Connection Management

```typescript
// ✅ GOOD: Global singleton with reconnection logic
import { io, Socket } from 'socket.io-client';

class SocketManager {
  private socket: Socket | null = null;
  
  connect() {
    if (this.socket?.connected) return this.socket;
    
    this.socket = io(process.env.NEXT_PUBLIC_API_URL!, {
      reconnection: true,
      reconnectionDelay: 1000,
      reconnectionDelayMax: 5000,
      reconnectionAttempts: Infinity,
    });
    
    this.socket.on('connect', () => {
      console.log('Socket connected');
    });
    
    this.socket.on('disconnect', () => {
      console.log('Socket disconnected, will auto-reconnect');
    });
    
    return this.socket;
  }
  
  disconnect() {
    this.socket?.disconnect();
    this.socket = null;
  }
}

export const socketManager = new SocketManager();
```

#### Rules for Dashboard
- Use Zustand for high-frequency state updates (not React Context)
- Throttle UI updates to 30-60 FPS max
- Use virtualized lists for any list > 50 items
- Implement socket reconnection with exponential backoff
- Import types from `@repo/dto` for type safety with backend

---

### 10. Node.js Streams Patterns (`stream-engine`)

#### Streaming Pipeline (Mandatory)

**CRITICAL:** FORBID `fs.readFileSync`, `readFile`, or loading entire arrays into memory.

```typescript
// ✅ GOOD: Streaming pipeline with backpressure
import { pipeline } from 'stream/promises';
import { createReadStream } from 'fs';
import csvParser from 'csv-parser';
import { Transform } from 'stream';

async function processCSV(filePath: string) {
  const validationTransform = new Transform({
    objectMode: true,
    transform(row, encoding, callback) {
      // Validate row
      if (isValidRow(row)) {
        this.push(formatForPostgres(row));
        callback();
      } else {
        // Skip invalid row, log it
        logger.warn('Invalid row', row);
        callback();
      }
    }
  });
  
  await pipeline(
    createReadStream(filePath),
    csvParser(),
    validationTransform,
    postgresWriteStream, // Use pg-copy-streams
  );
}

// ❌ BAD: Loading entire file into memory
async function processCSVBad(filePath: string) {
  const content = fs.readFileSync(filePath, 'utf-8'); // OOM on 2GB file
  const rows = content.split('\n');
  for (const row of rows) {
    await db.insert(row); // Also slow: N individual queries
  }
}
```

#### Backpressure Handling

```typescript
// ✅ GOOD: Respect backpressure signals
import { Writable } from 'stream';

class DatabaseWriter extends Writable {
  constructor(private db: PostgresConnection) {
    super({ objectMode: true, highWaterMark: 100 });
  }
  
  async _write(chunk: any, encoding: string, callback: Function) {
    try {
      await this.db.insert(chunk);
      callback(); // Signal ready for next chunk
    } catch (error) {
      callback(error); // Propagate error, stop pipeline
    }
  }
}

// Pipeline automatically pauses source when writer is slow
await pipeline(
  sourceStream,
  transformStream,
  new DatabaseWriter(db),
);
```

#### Postgres COPY Protocol (High Performance)

```typescript
// ✅ GOOD: Use COPY for bulk inserts
import copyFrom from 'pg-copy-streams';
import { pipeline } from 'stream/promises';
import { Transform } from 'stream';

async function bulkInsert(csvStream: NodeJS.ReadableStream) {
  const client = await pool.connect();
  
  try {
    const copyStream = client.query(
      copyFrom.from('COPY webhooks (provider, event_id, data) FROM STDIN CSV')
    );
    
    // Transform objects to CSV format
    const formatter = new Transform({
      objectMode: true,
      transform(obj, encoding, callback) {
        const csvLine = `${obj.provider},${obj.eventId},"${JSON.stringify(obj.data)}"\n`;
        callback(null, csvLine);
      }
    });
    
    await pipeline(csvStream, formatter, copyStream);
  } finally {
    client.release();
  }
}

// ❌ BAD: Individual INSERTs (100x slower)
async function bulkInsertBad(rows: any[]) {
  for (const row of rows) {
    await db.insert(webhooksTable).values(row); // Slow
  }
}
```

#### Memory-Constrained Environment

```typescript
// Docker Compose configuration for stream-engine
// deploy:
//   resources:
//     limits:
//       memory: 512M  # Hard limit

// ✅ GOOD: Monitor memory usage
import v8 from 'v8';

setInterval(() => {
  const heapStats = v8.getHeapStatistics();
  const usedMB = heapStats.used_heap_size / 1024 / 1024;
  
  if (usedMB > 400) {
    logger.warn('High memory usage', { usedMB });
  }
}, 5000);
```

#### Error Handling in Pipelines

```typescript
// ✅ GOOD: Catch errors at every stage
import { pipeline } from 'stream/promises';

try {
  await pipeline(
    sourceStream,
    transform1,
    transform2,
    destinationStream,
  );
  console.log('Pipeline completed successfully');
} catch (error) {
  // Pipeline automatically destroys all streams on error
  logger.error('Pipeline failed', error);
  throw error; // Propagate to caller
}

// ❌ BAD: Unhandled stream errors crash Node process
sourceStream
  .pipe(transform1)
  .pipe(transform2)
  .pipe(destinationStream);
// If any stream emits 'error' event without listener, process crashes
```

#### Rules for Stream Engine
- ALL file operations must use streams (no `fs.readFileSync`)
- Use `stream.pipeline()` for automatic cleanup and backpressure
- Use Postgres COPY protocol (not individual INSERTs)
- Monitor memory usage in 512MB container
- Handle errors at every pipeline stage
- Never load entire CSV into memory

