---
description: Standardized command reference for AI assistant interactions. Defines specific actions the AI should execute.
globs:
  - "**/*"
---

# Memory Bank Commands

Standardized commands for consistent AI assistant behavior. These commands define clear expectations for specific actions.

---

## Session Management Commands

### `/summarize`

**Purpose:** Start a new coding session with full context refresh.

**AI Actions:**
1. Read all Memory Bank files in `.cursor/rules/`:
   - `mb-active-context.mdc` (current state)
   - `mb-product-context.mdc` (business goals)
   - `mb-system-patterns.mdc` (technical patterns)
   - `mb-tech-stack.mdc` (technology stack)
2. Provide a concise summary:
   - Current project phase/focus
   - Recent changes from active context
   - Next steps checklist status
   - Open questions or blockers
3. Ask: "What would you like to work on?"

**Example:**
```
User: /summarize
AI: [Reads all mb-*.mdc files, provides summary]
```

---

### `/list-commands`

**Purpose:** Return the list of available Memory Bank commands in a lean format.

**Output format (lean):**
- One command per line: `/<command> - <purpose>`
- Keep it short: no examples, no extra commentary

**AI Actions:**
1. Read `.cursor/rules/mb-commands.mdc`
2. Extract all command headings (lines like `### \`/...\``)
3. Print the lean list in the order they appear

**Example output:**
```
/summarize - Start a new coding session with full context refresh.
/update-context - Update Memory Bank context files after significant work.
...
```

### `/update-context`

**Purpose:** Update Memory Bank context files after significant work.

**AI Actions:**
1. Ask user: "What context needs updating? [active | product | both]"
2. Based on response:
   - **active**: Update `mb-active-context.mdc`:
     - Add recent changes to table with today's date
     - Update next steps checklist
     - Update current focus if changed
     - Add any new architecture decisions
     - Update open questions/blockers
   - **product**: Update `mb-product-context.mdc`:
     - Add new user stories if applicable
     - Update KPIs if changed
     - Document new business requirements
   - **both**: Update both files as described above
3. Show diff of changes made
4. Confirm: "Context updated. Anything else to document?"

**Example:**
```
User: /update-context
AI: What context needs updating? [active | product | both]
User: active
AI: [Updates mb-active-context.mdc with recent session work]
```

---

## Investigation Commands

### `/investigate [topic/module/file]`

**Purpose:** Deep dive into existing code to understand implementation before making changes.

**AI Actions:**
1. Identify scope:
   - If module name: Find all files in that module
   - If file path: Focus on that specific file
   - If topic: Search codebase for relevant code
2. Perform analysis:
   - Read relevant files completely
   - Trace data flow and dependencies
   - Identify patterns and architecture used
   - Look for related tests
3. Provide structured report:
   - **Purpose:** What this code does
   - **Architecture:** How it's structured (classes, functions, modules)
   - **Dependencies:** What it depends on and what depends on it
   - **Patterns Used:** SOLID principles, design patterns observed
   - **Data Flow:** Input ‚Üí Processing ‚Üí Output
   - **Test Coverage:** What tests exist
   - **Observations:** Code quality, potential issues, improvement opportunities
4. Ask: "What would you like to do with this code?"

**Example:**
```
User: /investigate WebhookModule
AI: [Analyzes all WebhookModule files, provides detailed report]
```

---

### `/trace [function/flow]`

**Purpose:** Trace execution flow from entry point to completion.

**AI Actions:**
1. Identify entry point (controller, event handler, etc.)
2. Follow execution path step by step:
   - Function calls
   - Service injections
   - Database queries
   - Queue operations
   - Error handling paths
3. Create visual flow representation:
   ```
   POST /webhooks/:provider
   ‚Üì
   WebhookController.ingest()
   ‚Üì
   ValidationPipe validates CreateWebhookDto
   ‚Üì
   WebhookService.enqueue()
   ‚Üì
   BullMQ.add('webhooks', jobData)
   ‚Üì
   Return 202 Accepted
   ‚Üì (async)
   WebhookProcessor.handleJob()
   ‚Üì
   Check idempotency (Redis)
   ‚Üì
   Save to Database (Drizzle)
   ```
4. Highlight:
   - Async boundaries
   - Error handling points
   - External dependencies
   - Performance bottlenecks
5. Ask: "Do you need more detail on any step?"

**Example:**
```
User: /trace webhook ingestion flow
AI: [Traces complete flow from API to database with visual diagram]
```

---

## Code Quality Commands

### `/refactor [file/function]`

**Purpose:** Assist with code refactoring while maintaining behavior.

**AI Actions:**
1. Read target file/function completely
2. Analyze current implementation:
   - Identify code smells
   - Check against SOLID principles
   - Measure complexity (function length, nesting depth)
   - Find violations of `mb-system-patterns.mdc`
3. Propose refactoring strategy:
   - What needs to change and why
   - Suggested new structure
   - Risk assessment
4. Ask: "Would you like me to help with specific parts, or would you like to proceed with the refactoring yourself?"
5. If user requests help:
   - Provide guidance on specific techniques
   - Suggest function extraction points
   - Recommend interface definitions
   - **Do not implement unless explicitly asked**

**Example:**
```
User: /refactor src/webhook/webhook.controller.ts
AI: [Analyzes controller, identifies violations, proposes strategy]
AI: "Controller contains business logic in lines 45-67. Suggest extracting to WebhookValidationService. Would you like guidance on the extraction?"
```

---

### `/review [file/changes]`

**Purpose:** Review code for quality, patterns compliance, and best practices.

**AI Actions:**
1. Read target file or recent changes
2. Check against all standards:
   - **SOLID Principles** from `mb-system-patterns.mdc`
   - **TypeScript Strict Mode** compliance
   - **Error Handling** (Result Pattern vs try/catch)
   - **Async/Await** usage
   - **Documentation** (JSDoc presence and quality)
   - **NestJS Patterns** (proper separation of concerns)
   - **Code Efficiency** (data structures, algorithms)
3. Provide structured feedback:
   - ‚úÖ **Strengths:** What's done well
   - ‚ö†Ô∏è **Issues:** Problems found (with line numbers)
   - üí° **Suggestions:** Improvement opportunities
   - üìã **Checklist:** Remaining items from acceptance criteria
4. Prioritize feedback (critical ‚Üí important ‚Üí nice-to-have)

**Example:**
```
User: /review src/worker/webhook.processor.ts
AI: [Reviews against all standards, provides categorized feedback]
```

---

### `/lint-fix [scope]`

**Purpose:** Identify and help fix linter errors.

**AI Actions:**
1. Read linter errors for specified scope (file/directory/all)
2. Categorize errors:
   - TypeScript errors (type safety)
   - ESLint errors (code quality)
   - Import errors
   - Unused variables
3. For each error:
   - Explain what's wrong
   - Suggest fix approach
   - If user approves, apply fix
4. After fixes, verify no new errors introduced

**Example:**
```
User: /lint-fix src/webhook/
AI: [Reads lints, categorizes, suggests fixes one by one]
```

---

## Documentation Commands

### `/explain [concept/code]`

**Purpose:** Explain how something works in the context of this project.

**AI Actions:**
1. Identify what needs explanation:
   - If concept: Explain using project context
   - If code: Explain implementation and reasoning
2. Structure explanation:
   - **What:** High-level description
   - **Why:** Purpose and business value
   - **How:** Technical implementation
   - **Where:** Location in codebase
   - **Dependencies:** What it relies on
3. Use code examples from actual project
4. Reference relevant Memory Bank sections
5. Ask: "Does this clarify things, or would you like more detail?"

**Example:**
```
User: /explain idempotency checking
AI: [Explains concept with actual project implementation examples]
```

---

### `/document [file/function]`

**Purpose:** Help create or improve documentation.

**AI Actions:**
1. Read target code
2. Identify missing or inadequate documentation:
   - Missing JSDoc on public functions
   - Unclear parameter descriptions
   - Missing return type documentation
   - Missing usage examples
   - Complex logic without inline comments
3. Suggest documentation improvements:
   - Provide JSDoc template
   - Suggest description text
   - Identify edge cases to document
4. Ask: "Would you like to write the docs yourself, or should I generate templates?"

**Example:**
```
User: /document src/webhook/webhook.service.ts
AI: [Identifies missing JSDoc, suggests improvements]
```

---

## Testing Commands

### `/test-guide [feature/function]`

**Purpose:** Provide guidance on testing strategy.

**AI Actions:**
1. Analyze code to be tested
2. Identify test scenarios:
   - Happy path cases
   - Edge cases
   - Error scenarios
   - Integration points
3. Suggest test structure:
   - Test file organization
   - Mock requirements
   - Assertion points
4. Provide test skeleton (if requested):
   ```typescript
   describe('WebhookService', () => {
     describe('enqueue', () => {
       it('should add job to queue with valid DTO', async () => {
         // Arrange
         // Act
         // Assert
       });
       
       it('should handle queue connection failure', async () => {
         // Test error handling
       });
     });
   });
   ```
5. Ask: "Would you like help with specific test cases?"

**Example:**
```
User: /test-guide WebhookService.enqueue
AI: [Provides test scenarios and structure guidance]
```

---

## Architecture Commands

### `/architecture [module/system]`

**Purpose:** Explain or analyze architectural decisions and structure.

**AI Actions:**
1. Identify scope (specific module or entire system)
2. Analyze current architecture:
   - Module boundaries
   - Dependency flow
   - Data flow
   - Communication patterns
3. Create visual representation:
   - Module diagram
   - Dependency graph
   - Data flow diagram
4. Explain architectural patterns:
   - Why this structure
   - SOLID principles applied
   - Scalability considerations
   - Trade-offs made
5. Reference `mb-product-context.mdc` and `mb-system-patterns.mdc`

**Example:**
```
User: /architecture webhook ingestion
AI: [Provides comprehensive architectural overview with diagrams]
```

---

### `/dependencies [module/file]`

**Purpose:** Analyze and visualize dependencies.

**AI Actions:**
1. Identify target scope
2. Map dependencies:
   - **Imports:** What this code imports
   - **Exports:** What this code exports
   - **Dependents:** What depends on this code
   - **External:** Third-party packages used
3. Create dependency tree visualization
4. Analyze quality:
   - Circular dependencies (red flag)
   - Deep dependency chains (potential issue)
   - Tight coupling (refactor opportunity)
5. Suggest improvements if issues found

**Example:**
```
User: /dependencies WebhookModule
AI: [Maps and visualizes all dependencies, identifies issues]
```

---

## Performance Commands

### `/performance [file/function]`

**Purpose:** Analyze performance characteristics and suggest optimizations.

**AI Actions:**
1. Read target code
2. Identify performance concerns:
   - Synchronous I/O operations
   - Inefficient data structures (Array.find vs Map.get)
   - Sequential operations that could be parallel
   - Memory leaks (missing cleanup)
   - N+1 query patterns
3. Measure complexity:
   - Time complexity (Big O)
   - Space complexity
4. Suggest optimizations:
   - Convert to async/await if synchronous
   - Use Promise.all for parallelization
   - Better data structures
   - Caching opportunities
5. Reference `mb-system-patterns.mdc` performance guidelines

**Example:**
```
User: /performance webhook processing
AI: [Analyzes performance, suggests specific optimizations]
```

---

## Search Commands

### `/find [pattern/concept]`

**Purpose:** Search codebase for specific patterns or concepts.

**AI Actions:**
1. Understand search intent
2. Use appropriate search strategy:
   - **Exact text:** Use grep
   - **Semantic:** Use codebase_search
   - **File name:** Use glob_file_search
3. Present results grouped by:
   - File location
   - Relevance
   - Module
4. Provide context for each result
5. Ask: "Would you like to investigate any of these further?"

**Example:**
```
User: /find error handling patterns
AI: [Searches codebase, presents categorized results]
```

---

## Git Commands

### `/changes`

**Purpose:** Summarize recent changes and their impact.

**AI Actions:**
1. Review git diff or recent commits
2. Categorize changes:
   - New features
   - Bug fixes
   - Refactoring
   - Documentation
3. Analyze impact:
   - Files modified
   - Lines changed
   - Tests affected
   - Breaking changes
4. Verify against standards:
   - Follows `mb-system-patterns.mdc`
   - Maintains type safety
   - Proper error handling
5. Suggest next steps (tests, documentation, etc.)

**Example:**
```
User: /changes
AI: [Summarizes recent changes with impact analysis]
```

---

### `/commit [--type=<type>] [--scope=<scope>] [--message="<msg>"] [--all] [--no-verify]`

**Purpose:** Commit changes using a semantic (Conventional Commits) message.

**Semantic format (Conventional Commits):**
- `<type>(<scope>): <description>`
- `<type>: <description>` (scope optional)
- Optional **breaking change**:
  - `type(scope)!: description` OR
  - Footer: `BREAKING CHANGE: <explanation>`

**Common types:** `feat`, `fix`, `chore`, `docs`, `refactor`, `test`, `perf`, `ci`, `build`, `revert`

**AI Actions:**
1. Inspect repo state:
   - Run `git status`
   - Summarize changes (prefer `git diff --stat`; distinguish staged vs unstaged when relevant)
2. Propose semantic commit messages:
   - Suggest 1‚Äì3 Conventional Commit messages based on the diff
   - Recommend splitting into multiple commits if changes span unrelated concerns
3. Confirm intent before mutating git state:
   - Ask which message to use (or accept `--message`)
   - Stage changes:
     - If `--all` is set: stage with `git add -A`
     - Otherwise: ask what to stage (or proceed only if changes are already staged)
   - Ask for final confirmation to run `git commit`
4. Create commit:
   - Run `git commit -m "<semantic message>"` (append `--no-verify` only if requested)
5. Report outcome:
   - Echo final commit message used
   - Show `git status` to confirm result

**Safety Rules:**
- If there are no changes, do not commit; explain why.
- If the working tree includes unrelated changes, prefer a narrower commit or multiple commits.
- Never push or tag unless explicitly instructed.

**Examples:**
```
User: /commit --type=feat --scope=webhook
AI: Proposed: "feat(webhook): add signature verification for ProviderX" ... [asks to confirm + stage + commit]
```

```
User: /commit --message="fix(queue): handle retry backoff overflow"
AI: [verifies diff matches message, asks to confirm + stage + commit]
```

---

## Workflow Commands

### `/plan [task]`

**Purpose:** Help plan implementation approach without generating code.

**AI Actions:**
1. Understand the task
2. Investigate relevant existing code
3. Create implementation plan:
   - **Phase 1:** Preparation (what to read, what to understand)
   - **Phase 2:** Core implementation (what files to modify)
   - **Phase 3:** Integration (how to connect pieces)
   - **Phase 4:** Testing (what to test)
4. Identify risks and challenges
5. Reference relevant Memory Bank patterns
6. Ask: "Does this plan make sense? What would you like to tackle first?"

**Example:**
```
User: /plan add webhook retry mechanism
AI: [Provides step-by-step implementation plan without code]
```

---

### `/checkpoint`

**Purpose:** Create a checkpoint of current work state.

**AI Actions:**
1. Summarize work done in current session:
   - Files modified
   - Features added/fixed
   - Tests written
   - Documentation updated
2. Suggest context updates:
   - Should `mb-active-context.mdc` be updated?
   - Should `mb-product-context.mdc` be updated?
3. Verify completion:
   - Are there TODO items to create?
   - Are there open questions to document?
   - Are there follow-up tasks?
4. Recommend next steps

**Example:**
```
User: /checkpoint
AI: [Summarizes session, suggests context updates, identifies next steps]
```

---

## Command Usage Guidelines

### When to Use Commands

**Use commands when you want:**
- Structured, predictable AI behavior
- To learn and understand (not just get solutions)
- To maintain control over implementation
- To follow established patterns consistently

### Command Composition

Commands can be combined:
```
User: /summarize then /investigate WebhookModule
AI: [Executes both in sequence]
```

### Custom Parameters

Add specifics to commands:
```
User: /review src/webhook/ --focus=error-handling
AI: [Reviews webhook module focusing on error handling]
```

---

## AI Behavior Guidelines

### Core Principles

1. **Teach, Don't Do:** Explain and guide rather than implementing (unless explicitly asked)
2. **Context-Aware:** Always reference Memory Bank files for standards
3. **Ask, Don't Assume:** When unclear, ask for clarification
4. **Concise but Complete:** Be thorough but respect user's time
5. **Code Second:** Only generate code when user explicitly requests it

### Response Format

All command responses should:
- Start with clear section headers
- Use bullet points for lists
- Include code examples from actual project (not generic)
- Reference Memory Bank files when relevant
- End with a clarifying question or next step

### What NOT to Do

- ‚ùå Don't generate full implementations unprompted
- ‚ùå Don't update files without explicit permission
- ‚ùå Don't add dependencies without approval
- ‚ùå Don't commit changes without explicit instruction
- ‚ùå Don't assume requirements - ask questions
